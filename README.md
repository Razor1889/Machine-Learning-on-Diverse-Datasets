Machine Learning on Diverse Datasets
This repository contains the codebase for my project on applying machine learning methodologies to train models and evaluate their effectiveness across different computer architectures.
Project Overview
This project presents a comparative analysis of the efficiency of Central Processing Units (CPUs) and Graphics Processing Units (GPUs) in executing complex machine learning algorithms involving both numerical and categorical data.
CPUs are known for their versatility and ability to sequentially process complex logic.


GPUs leverage parallel processing through their multicore architecture, offering significant performance advantages in certain machine learning tasks.


The project evaluates how each processing architecture performs when executing machine learning code. Performance metrics such as execution time and resource utilization are analyzed to assess architectural strengths and weaknesses. These findings provide practical guidance on when to use CPUs versus GPUs for specific machine learning workloads.
Evaluation Strategy
The project also includes an evaluation of how dataset types affect the correctness and performance of machine learning models. Two primary strategies were used:
Train-and-Test Strategy
Selective Cross-Validation Strategy


The impact of these strategies is observed across different datasets, with attention paid to the resulting performance metrics. The analysis highlights:
The comparative strengths and weaknesses of each validation method. Insights into appropriate applications of each strategy.
How changes in individual metrics affect model accuracy.


Keywords
Machine learning, graphics processing unit (GPU), central processing unit (CPU), efficiency, train-and-test, selective cross-validation

